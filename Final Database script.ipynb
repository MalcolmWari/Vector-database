{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a transform to normalize the data. The transforms are applied\n",
    "# in the order they are given. First, the image is converted to a tensor,\n",
    "# then it is normalized with mean and standard deviation of 0.5 for all channels.\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Download the CIFAR-10 dataset, apply the transforms, and load it into\n",
    "# a DataLoader for efficient batch processing and shuffling.\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "# Define the classes in the CIFAR-10 dataset. These correspond to the labels\n",
    "# of the images in the dataset.\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Load the pretrained ResNet-50 model.\n",
    "net = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze all the parameters of the model, making them untrainable.\n",
    "# This is done because we want to keep the weights of the pre-trained model\n",
    "# and only train the final layer that we will add next.\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the last fully connected layer of the model with a new one \n",
    "# having the correct number of output features. The new layer's parameters\n",
    "# are not frozen, so they will be learned during training.\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "# Define a loss function and an optimizer. The loss function is used to measure\n",
    "# how well the model's predictions match the actual labels, and the optimizer\n",
    "# is used to update the model's parameters based on this loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device (GPU or CPU)\n",
    "net.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs; data is a list of [inputs, labels]\n",
    "        # Move the inputs and labels to the device\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        # This is necessary because by default, gradients are accumulated\n",
    "        # in backward passes, so we need to clear them at each step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute the outputs by passing inputs to the model\n",
    "        outputs = net(inputs)\n",
    "        # Compute the loss between the outputs and the ground truth labels\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass: compute the gradients of the loss w.r.t. the model's parameters\n",
    "        loss.backward()\n",
    "        # Perform an optimization step: update the model's parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        # Print every 2000 mini-batches\n",
    "        if i % 2000 == 1999:    \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# After training, we want to test how well the model performs on unseen data\n",
    "# We'll compute the accuracy of the model on the test data\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# We don't need to compute gradients during testing, so we use torch.no_grad() \n",
    "# to disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        # Get the inputs; data is a list of [inputs, labels]\n",
    "        # Move the inputs and labels to the device\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # Forward pass: compute the outputs by passing inputs to the model\n",
    "        outputs = net(images)\n",
    "        # Get the predicted class by finding the maximum value \n",
    "        # (since we're using CrossEntropyLoss)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # Update the total number of images\n",
    "        total += labels.size(0)\n",
    "        # Update the number of correctly predicted images\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# We will store the vectors in a Python dictionary. \n",
    "# The keys will be the image labels and the values will be the vectors.\n",
    "vectors = {}\n",
    "\n",
    "# We move the model to the CPU for the feature extraction. \n",
    "# This is because we're going to be working with numpy, which can't handle CUDA tensors.\n",
    "net = net.to('cpu')\n",
    "\n",
    "# We don't need to compute gradients during feature extraction, \n",
    "# so we use torch.no_grad() to disable gradient computation.\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        # Get the inputs; data is a list of [inputs, labels]\n",
    "        images, labels = data\n",
    "        # Forward pass: compute the outputs by passing inputs to the model\n",
    "        outputs = net(images)\n",
    "        # For each image in the batch, we store its vector in the dictionary.\n",
    "        for i in range(len(outputs)):\n",
    "            vectors[labels[i].item()] = outputs[i].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malco\\mambaforge\\envs\\data\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malco\\mambaforge\\envs\\data\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the ResNet-50 model architecture\n",
    "net = models.resnet50(pretrained=False)  # Note: pretrained is set to False\n",
    "\n",
    "# Modify the final layer to match CIFAR-10's 10 classes\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "# Load the saved model weights\n",
    "saved_weights_path = \"C:/Users/Malco/OneDrive/Desktop/Vector Database Project/cifar_net.pth\"\n",
    "net.load_state_dict(torch.load(saved_weights_path))\n",
    "\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: dog\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Get a random image\n",
    "dataiter = iter(trainloader)\n",
    "image, label = next(dataiter)\n",
    "\n",
    "# Function to show image\n",
    "def imshow(img):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "# Display the image\n",
    "imshow(image[0])\n",
    "\n",
    "print(\"Label:\", trainset.classes[label[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vector = image.view(-1)\n",
    "\n",
    "torch.save(image_vector, 'cifar_vector.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Running on device: cuda:0\n",
      "Model weights loaded successfully and moved to device!\n",
      "Extracted 5000 feature vectors for class 0.\n",
      "Extracted 5000 feature vectors for class 1.\n",
      "Extracted 5000 feature vectors for class 2.\n",
      "Extracted 5000 feature vectors for class 3.\n",
      "Extracted 5000 feature vectors for class 4.\n",
      "Extracted 5000 feature vectors for class 5.\n",
      "Extracted 5000 feature vectors for class 6.\n",
      "Extracted 5000 feature vectors for class 7.\n",
      "Extracted 5000 feature vectors for class 8.\n",
      "Extracted 5000 feature vectors for class 9.\n",
      "Feature vectors serialized and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "# 1. Load the CIFAR-10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "# Check for GPU availability and set the device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# 2. Set up the ResNet-50 model structure\n",
    "net = models.resnet50(pretrained=False)\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "# 3. Load the saved model weights and move the model to the device\n",
    "weights_path = \"cifar_net.pth\"\n",
    "net.load_state_dict(torch.load(weights_path))\n",
    "net.to(device)\n",
    "print(\"Model weights loaded successfully and moved to device!\")\n",
    "\n",
    "# 4. Extract feature vectors for all images in the dataset\n",
    "vectors = {label: [] for label in range(10)}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        outputs = outputs.cpu()\n",
    "        for i in range(len(outputs)):\n",
    "            label = labels[i].item()\n",
    "            vectors[label].append(outputs[i].numpy())\n",
    "\n",
    "# Check the number of vectors extracted for each class\n",
    "for label, vecs in vectors.items():\n",
    "    print(f\"Extracted {len(vecs)} feature vectors for class {label}.\")\n",
    "\n",
    "# 5. Serialize and save the 'vectors' dictionary\n",
    "with open(\"vectors.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectors, f)\n",
    "\n",
    "print(\"Feature vectors serialized and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malco\\mambaforge\\envs\\data\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Malco\\mambaforge\\envs\\data\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vectors extracted, serialized, and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import pickle\n",
    "\n",
    "# 1. Load the CIFAR-10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "# Check for GPU availability and set the device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Set up the ResNet-50 model structure\n",
    "net = models.resnet50(pretrained=False)\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "# Load the saved model weights and move the model to the device\n",
    "weights_path = \"cifar_net.pth\"\n",
    "net.load_state_dict(torch.load(weights_path))\n",
    "net.to(device)\n",
    "net.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# 3. Extract feature vectors\n",
    "all_vectors = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        outputs = outputs.cpu()\n",
    "        for i in range(len(outputs)):\n",
    "            # Each entry will be a tuple of (label, feature vector)\n",
    "            all_vectors.append((labels[i].item(), outputs[i].numpy()))\n",
    "\n",
    "# 4. Serialize and save the 'all_vectors' list\n",
    "with open(\"vectors.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_vectors, f)\n",
    "\n",
    "print(\"Feature vectors extracted, serialized, and saved successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vectors serialized and saved to vectors.h5!\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have 'all_vectors' from the previous extraction step\n",
    "\n",
    "# 1. Organize the data for serialization\n",
    "labels = [vec[0] for vec in all_vectors]\n",
    "vectors = [vec[1] for vec in all_vectors]\n",
    "labels_array = np.array(labels)\n",
    "vectors_array = np.array(vectors)\n",
    "\n",
    "# 2. Serialize the feature vectors using HDF5\n",
    "with h5py.File('vectors.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"labels\", data=labels_array)\n",
    "    hf.create_dataset(\"vectors\", data=vectors_array)\n",
    "\n",
    "print(\"Feature vectors serialized and saved to vectors.h5!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 3072 while Y.shape[1] == 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Malco\\OneDrive\\Desktop\\Vector Database Project\\Final Database script.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Malco/OneDrive/Desktop/Vector%20Database%20Project/Final%20Database%20script.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m image_vector_np \u001b[39m=\u001b[39m image_vector\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Malco/OneDrive/Desktop/Vector%20Database%20Project/Final%20Database%20script.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# 2. Compute the Cosine Similarities\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Malco/OneDrive/Desktop/Vector%20Database%20Project/Final%20Database%20script.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m similarities \u001b[39m=\u001b[39m cosine_similarity(image_vector_np, vectors_h5)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Malco/OneDrive/Desktop/Vector%20Database%20Project/Final%20Database%20script.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# 3. Determine the Most Similar Image\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Malco/OneDrive/Desktop/Vector%20Database%20Project/Final%20Database%20script.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m most_similar_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(similarities)\n",
      "File \u001b[1;32mc:\\Users\\Malco\\mambaforge\\envs\\data\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Malco\\mambaforge\\envs\\data\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1577\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1542\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1543\u001b[0m \n\u001b[0;32m   1544\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1573\u001b[0m \u001b[39m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1577\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[0;32m   1579\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1580\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\Malco\\mambaforge\\envs\\data\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:190\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    185\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPrecomputed metric requires shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    186\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(n_queries, n_indexed). Got (\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfor \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m indexed.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[0;32m    188\u001b[0m         )\n\u001b[0;32m    189\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mX.shape[1] == \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m while Y.shape[1] == \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m    193\u001b[0m     )\n\u001b[0;32m    195\u001b[0m \u001b[39mreturn\u001b[39;00m X, Y\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 3072 while Y.shape[1] == 10"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. Load the Data\n",
    "with h5py.File('vectors.h5', 'r') as hf:\n",
    "    labels_h5 = np.array(hf[\"labels\"])\n",
    "    vectors_h5 = np.array(hf[\"vectors\"])\n",
    "\n",
    "# Load the CIFAR-10 image vector\n",
    "# Replace 'your_path' with the path to your saved CIFAR-10 image vector\n",
    "with open(\"C:/Users/Malco/OneDrive/Desktop/Vector Database Project/cifar_vector.pth\", 'rb') as f:\n",
    "    image_vector = torch.load(f)\n",
    "image_vector_np = image_vector.numpy().reshape(1, -1)\n",
    "\n",
    "# 2. Compute the Cosine Similarities\n",
    "similarities = cosine_similarity(image_vector_np, vectors_h5)\n",
    "\n",
    "# 3. Determine the Most Similar Image\n",
    "most_similar_index = np.argmax(similarities)\n",
    "most_similar_label = labels_h5[most_similar_index]\n",
    "most_similar_value = similarities[0][most_similar_index]\n",
    "\n",
    "print(f\"Most similar label: {most_similar_label}, Similarity value: {most_similar_value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
